K-Nearest Neighbors (KNN) Classification â€“ Iris Dataset

A simple yet complete machine learning project demonstrating KNN Classification using the classic Iris flower dataset.

ğŸ“‚ Project Structure
ğŸ“ KNN-Classification
â”‚â”€â”€ KNN_Iris_Notebook.ipynb
â”‚â”€â”€ Iris.csv
â”‚â”€â”€ README.md

ğŸ§  Project Overview

This project implements the K-Nearest Neighbors (KNN) algorithm to classify iris flower species based on their measured features.

ğŸ” Key Steps

Load & explore the dataset

Data preprocessing (scaling, feature selection)

Train-test split

Test multiple K values

Select best K

Evaluate using accuracy & confusion matrix

Plot K vs Accuracy

ğŸ› ï¸ Technologies Used

Python

Pandas

NumPy

Scikit-learn

Matplotlib

ğŸ“Š Dataset

The project uses the Iris Dataset, which contains:

150 samples

4 numerical features

3 flower species

Target: Species classification

Dataset File: Iris.csv

ğŸš€ How to Run
1ï¸âƒ£ Install Dependencies
pip install pandas numpy scikit-learn matplotlib

2ï¸âƒ£ Run the Notebook

Open the notebook:

jupyter notebook KNN_Iris_Notebook.ipynb

ğŸ§ª Model Training Code (Summary)
from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=best_k)
model.fit(X_train, y_train)
pred = model.predict(X_test)

ğŸ“ˆ Results
âœ” K vs Accuracy Graph

Shows how accuracy changes for K = 1â€“10.

âœ” Best K

best_k = 3 (also 5â€“10 achieve 100%)

âœ” Confusion Matrix

Perfect classification:

[[19  0  0]
 [ 0 13  0]
 [ 0  0 13]]

ğŸ“š What Youâ€™ll Learn

Instance-based learning

Euclidean distance

Choosing optimal K

How KNN works

Feature scaling importance

ğŸ“ Future Enhancements

Add decision boundary visualization

Add hyperparameter tuning using GridSearchCV

Deploy model using Flask/Streamlit

Convert project into a full ML pipeline

